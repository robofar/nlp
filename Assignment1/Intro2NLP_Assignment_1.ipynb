{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a325897",
   "metadata": {
    "id": "5a325897",
    "tags": []
   },
   "source": [
    "# Introduction to Natural Language Processing: Assignment 1\n",
    "\n",
    "In this assignment we'll practice tokenization, lemmatization and stemming\n",
    "\n",
    "- Please comment your code\n",
    "- Submissions are due Thursday at 23:59 and should be submitted **ONLY** on eCampus: **Assignmnets >> Student Submissions >> Assignment 1 (Deadline: 14.11.2023, at 23:59)**\n",
    "- Name the file aproppriately \"Assignment_1_\\<Your_Name\\>.ipynb\".\n",
    "- Please submit **ONLY** the Jupyter Notebook file.\n",
    "- Please use relative path; Your code should work on my computer if the Jupyter Notebook and the file are both in the same directory.\n",
    "\n",
    "Example: file_name = lemmatization-en.txt >> **DON'T use:** /Users/ComputerName/Username/Documents/.../lemmatization-en.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "991ae01f-d760-4adf-a8bd-7e7c56fe5d06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3429f89-de75-42e5-aefc-1baff40e3def",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6c2dcc-14f5-4e6b-b778-1a3831e792a4",
   "metadata": {},
   "source": [
    "$\\textbf{Tokenization}$ is the process of creating $\\textbf{Tokens}$.\n",
    "\n",
    "$\\textbf{Tokens}$ are building units of text sequence.\n",
    "\n",
    "Simply said, units that build up the text corpus are $\\textbf{tokens}$ and the process of splitting a text sequence into its tokens is $\\textbf{tokenization}$.\n",
    "\n",
    "$\\textbf{Tokens}$ can be (depending on our task and goal):\n",
    "   - characters\n",
    "   - words (individual words or sets of multiple words together)\n",
    "   - part of words\n",
    "   - punctuations\n",
    "   - sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfa778e-c628-4510-9ba1-27859181a414",
   "metadata": {},
   "source": [
    "## Types of Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad243fa7-a3e4-42ed-ba23-1456b5f6d778",
   "metadata": {},
   "source": [
    "There are 3 main types of tokenization:\n",
    "   - Word Tokenization - splits text corpus into words\n",
    "   - Sentence Tokenization - splits text corpus into sentences\n",
    "   - Character Tokenization - splits text corpus into individual characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1694eea7-1b21-4f41-8220-f80c9f59497f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cd8bf33",
   "metadata": {
    "id": "0cd8bf33"
   },
   "source": [
    "### Task 1.1 (3 points)\n",
    "\n",
    "Write a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n",
    "1. num_words: The number of words in string\n",
    "2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n",
    "\n",
    "**Hint:** The string can be a single word or a sentence and\n",
    " can contain some special charecters, such as: \"!\", \",\", \":\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacdd501-1093-488e-b5db-57b40f0982c4",
   "metadata": {},
   "source": [
    "Function for creating my tokens is given below. I used $re$ module, and its function $split$. So, splitting of sentence, will be done based on provided regex. Let me explain written regex:\n",
    "   - \\[ \\] - it allows us to denote all of the characters that we want to split based on (basically group them together)\n",
    "   - \\s!@.:;,# - list of characters that we want to split based on. For convenicence, \\s means one space\n",
    "   - \\+ - means one or more occurences of preceeding character. Preceeding character is actually a list of characters (here you can unterstand more what [] does actually)\n",
    "   \n",
    "So this will split based on 1+ occurences of all of the characters that appear between the square brackets. Obviously, much complex regex could be written to cover many more cases and special characters, and that would be only the extension of already written regex. \n",
    "\n",
    "The only problem with this regex is so-called $\\textbf{trailing dot}$. $\\textbf{trailing dot}$ means dot that appears as a last character in sentence. Problem is, that it will split based on this $\\textbf{trailing dot}$, but since there is nothing behind that dot, it will return '' (empty space) as a token. Since trailing dot can only appear at the end, and it will be last in the list of returned split, we can just return all elements except last one (which is that dot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f14f3124",
   "metadata": {
    "id": "f14f3124",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_words_tokens(any_string, regex):\n",
    "    word_list = re.split(regex, any_string)[:-1]\n",
    "    num_words = len(word_list)\n",
    "    tokens = sorted(list(set(word_list))) # unique elements in this list\n",
    "    num_tokens = len(tokens)\n",
    "    return word_list, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dad39d8-9bc5-4b5c-8099-464d4d3bb559",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words:  14\n",
      "Number of tokens:  10\n"
     ]
    }
   ],
   "source": [
    "regex = r'[\\s!@.:;,#]+'\n",
    "words, tokens = extract_words_tokens(\"This is is is a sample text for. testing RegexpTokenizer in NLTK.NLTK.NLTK.\", regex)\n",
    "print('Number of words: ', len(words))\n",
    "print('Number of tokens: ', len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc30a9a-faa6-4d90-a28d-0eb2073af851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37c22e8-1509-4ffb-8e76-ec07471e38f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d67dab96-929d-4ccf-aa72-7fc0f1768120",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b05add",
   "metadata": {
    "id": "a4b05add"
   },
   "source": [
    "### Task 1.2 (4 points)\n",
    "\n",
    "Write a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n",
    "\n",
    "**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ed01ad-f111-425e-820c-35529a1e8676",
   "metadata": {},
   "source": [
    "### Note:\n",
    "It is written dictionary of all $\\textbf{words}$, but since there can be many same words (which are basically represented by same token) that would mean that we want to create dictionary where all those words are the keys. But since key must be unique in dictionary, I suppose you wanted to say - dictionary with all $\\textbf{tokens}$ as keys and the $\\textbf{lemma}$ of the $\\textbf{tokens}$ as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a12f48ff",
   "metadata": {
    "id": "a12f48ff"
   },
   "outputs": [],
   "source": [
    "def lemmatize(any_string, file_name):\n",
    "    df = pd.read_csv(file_name, sep='\\t', header=None, names=['lemma', 'token'])\n",
    "    # Set the 'token' column as the index for easier search (token is unique, whereas lemma is not)\n",
    "    df.set_index('token', inplace=True)\n",
    "    \n",
    "    # Regex meaning: split based on:\n",
    "    # 1) [\\s]+ -> one or multiple occurences of whitespace , or (|)\n",
    "    # 2) $ -> end of the line\n",
    "    regex = r'[\\s]+|$'\n",
    "    words, tokens = extract_words_tokens(any_string, regex)\n",
    "    print(words)\n",
    "    \n",
    "    dictionary_of_lemmatized_tokens = {}\n",
    "    for token in tokens:\n",
    "        dictionary_of_lemmatized_tokens[token] = df.loc[token]['lemma']\n",
    "        \n",
    "    return dictionary_of_lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff8eeb26-a8ff-446a-a1d5-9f7e5fc3c7de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_name = \"lemmatization-en.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ff62722-092b-46b4-b3ed-3a9c07bbb065",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bustards', 'busies', 'acclimated', 'acclimates', 'acclimating']\n"
     ]
    }
   ],
   "source": [
    "dictionary_of_lemmatized_tokens = lemmatize(\"bustards busies acclimated acclimates acclimating\", file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91b5d793-92a4-4ebb-a9ab-bf88e3af818c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acclimated': 'acclimate',\n",
       " 'acclimates': 'acclimate',\n",
       " 'acclimating': 'acclimate',\n",
       " 'busies': 'busy',\n",
       " 'bustards': 'bustard'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_of_lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660bc766-f3a9-4a2f-9f91-c26c6712bb83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b718800-1879-43b7-8581-32fd52874dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21627a18-8485-47c7-88f7-b57cc37bf579",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f266bdc4",
   "metadata": {
    "id": "f266bdc4"
   },
   "source": [
    "### Task 1.3 (3 points)\n",
    "\n",
    "Write a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\n",
    "\n",
    "Create rules for the following forms of the verbs, Here is one example:\n",
    "\n",
    "- (Infinitive form) >> study - studi\n",
    "- (Present simple tense: Third person) >> studies - studi\n",
    "- (Continuous tense) >> studying - studi\n",
    "- (Past simple tense) >> studied - studi\n",
    "\n",
    "**Hint:** The string can be a single word or a sentence and\n",
    " can contain some special charecters, such as: \"!\", \",\", \":\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d182830f-cc14-4f2f-8924-74f4c90a1073",
   "metadata": {},
   "source": [
    "Same as in lemmatization, goal is to get base form of the word. When it comes to stemming, it is simpler process then lemmatization, since in stemming base word does not need to have some meaning itself (whereas in lemmatization base form need to be some meaningful word). So, main process of stemming is to find set of rules, that maps word to its stem. This is not simple process, so we can only mimic stemming process by defining some set of naive rules, for the purpose of showing that we understand the main idea of how stemming works.\n",
    "\n",
    "So for that purpose, I wrote set of naive rules, where you can give one word as input, output will be its stem. I printes examples for few words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b5c587b",
   "metadata": {
    "id": "0b5c587b"
   },
   "outputs": [],
   "source": [
    "def stemmer(any_string):\n",
    "    # Stemming rules as regex\n",
    "    rules = {\n",
    "        \"y$\" : 'i',\n",
    "        \"ies$\" : 'i',\n",
    "        \"ying$\" : 'i',\n",
    "        \"ed$\" : 'i',\n",
    "        \"ing$\": \"\",\n",
    "        \"ed$\": \"\",\n",
    "        \"ves$\": \"f\",\n",
    "        \"ied$\": \"y\",\n",
    "        \"er$\": \"\",\n",
    "        \"est$\": \"\",\n",
    "        \"en$\": \"\",\n",
    "        \"ly$\": \"\",\n",
    "        \"ful$\": \"\",\n",
    "        \"ment$\": \"\",\n",
    "        \"ness$\": \"\",\n",
    "        \"able$\": \"\",\n",
    "        \"ize$\": \"\",\n",
    "        \"ise$\": \"\",\n",
    "        \"ation$\": \"\",\n",
    "        \"ator$\": \"\",\n",
    "        \"ative$\": \"\",\n",
    "        \"al$\": \"\",\n",
    "        \"ence$\": \"\",\n",
    "        \"ance$\": \"\",\n",
    "        \"tion$\": \"\",\n",
    "        \"ion$\": \"\",\n",
    "        \"ity$\": \"\",\n",
    "        \"ous$\": \"\",\n",
    "        \"ify$\": \"\",\n",
    "        \"ible$\": \"\",\n",
    "        \"ism$\": \"\",\n",
    "        \"ist$\": \"\",\n",
    "        \"ite$\": \"\",\n",
    "        \"ship$\": \"\",\n",
    "        \"hood$\": \"\"\n",
    "    }\n",
    "\n",
    "    \n",
    "    stemmed_string = any_string\n",
    "    for rule, replacement in rules.items():\n",
    "        pattern = re.compile(rule)\n",
    "        if re.search(pattern, any_string):\n",
    "            stemmed_string = re.sub(pattern, replacement, any_string)\n",
    "            \n",
    "    return stemmed_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d692028-8a38-49b3-be11-f91bb2e1b2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "studi\n",
      "neighbour\n",
      "likeli\n",
      "f\n",
      "st\n",
      "crazi\n",
      "plai\n",
      "plays\n",
      "play\n",
      "play\n"
     ]
    }
   ],
   "source": [
    "print(stemmer('studies'))\n",
    "print(stemmer('neighbourhood'))\n",
    "print(stemmer('likelihood'))\n",
    "print(stemmer('fence'))\n",
    "print(stemmer('stance'))\n",
    "print(stemmer('crazy'))\n",
    "\n",
    "print(stemmer('play'))\n",
    "print(stemmer('plays'))\n",
    "print(stemmer('playing'))\n",
    "print(stemmer('played'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a8d218-ceb1-4dd2-b5b5-70eb7671ebf0",
   "metadata": {},
   "source": [
    "Results are not too good, but logic of how stemming works is implemented. Having (proper) set of rules, we would be able to convert word to its corresponding stem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a362b0-e2e9-4ce7-a610-57f57e3038de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
