{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing\n",
    "\n",
    "### Tutorial 4\n",
    "\n",
    "---\n",
    "\n",
    "This tutorial will show how to use spaCy to obtain features that we have been extracting using a rule-based approach on pure Python and how to use spaCy to get word vectors.\n",
    "\n",
    "### spaCy\n",
    "\n",
    "is an open-source library for advanced NLP in Python, which supports a wide variety of languages. One crucial advantage of using spaCy is that it's designed to be integrated into real-world products without serious difficulties.\n",
    "\n",
    "\n",
    "To begin working with spaCy, we need to specify which language class we want to use. Remember that spaCy was created to be used for several languages (currently 64+ languages and 55 trained pipelines for 17 languages). It can't assume that we want to use English. We need to specify this explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "\n",
    "If you haven't intalled spaCy, please run the following line in a separate cell\n",
    "\n",
    "`!pip install spacy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin with an example in English. Since we already know how to tokenize a text, let's take a look of how spaCy does this process for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences = ['I always take tea for breakfast.',\n",
    "             'It was not my intent to break something.',\n",
    "             'Bild.de can update content for the channel on-the-fly and thus ensure the neartime presentation of breaking news and information.',\n",
    "             'We have enough time to take a break.',\n",
    "             'In case the drive direction is changed the outputs for up and down will be switched off during break time. ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to extract only the sentences that contain the word: \"break\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I always take tea for breakfast.',\n",
       " 'It was not my intent to break something.',\n",
       " 'Bild.de can update content for the channel on-the-fly and thus ensure the neartime presentation of breaking news and information.',\n",
       " 'We have enough time to take a break.',\n",
       " 'In case the drive direction is changed the outputs for up and down will be switched off during break time. ']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def has_break(text):\n",
    "    return \"break\" in text\n",
    "\n",
    "g = (sentence for sentence in sentences if has_break(sentence))\n",
    "print(type(g))\n",
    "\n",
    "[next(g) for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import English\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "# One sentence/document\n",
    "raw = \"Hard to judge whether, these sides were good. We were grossed \" \\\n",
    "      \"out by the melted styrofoam and didn't want to eat it for fear of getting sick.\"\n",
    "\n",
    "doc = nlp(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard to judge whether, these sides were good. We were grossed out by the melted styrofoam and didn't want to eat it for fear of getting sick.\n"
     ]
    }
   ],
   "source": [
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard\n",
      "to\n",
      "judge\n",
      "whether\n",
      ",\n",
      "these\n",
      "sides\n",
      "were\n",
      "good\n",
      ".\n",
      "We\n",
      "were\n",
      "grossed\n",
      "out\n",
      "by\n",
      "the\n",
      "melted\n",
      "styrofoam\n",
      "and\n",
      "did\n",
      "n't\n",
      "want\n",
      "to\n",
      "eat\n",
      "it\n",
      "for\n",
      "fear\n",
      "of\n",
      "getting\n",
      "sick\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿\n",
      "Es\n",
      "posible\n",
      "\"\n",
      "desconectar\n",
      "\"\n",
      "a\n",
      "un\n",
      "país\n",
      "entero\n",
      "de\n",
      "internet\n",
      "?\n",
      "La\n",
      "respuesta\n",
      "corta\n",
      "es\n",
      "\"\n",
      "sí\n",
      "\"\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Now it's your turn to do the same for the following Spanish text taken from BBC in Spanish.\n",
    "from spacy.lang.es import Spanish \n",
    " \n",
    "nlp = Spanish()\n",
    " \n",
    "spanish_raw = '¿Es posible \"desconectar\" a un país entero de internet? ' \\\n",
    "              'La respuesta corta es \"sí\".'\n",
    " \n",
    "document = nlp(spanish_raw)\n",
    " \n",
    "for token in document:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "\n",
    "spaCy uses the same syntax as Python for indexing. This way you can address specific tokens in your documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "Space can also be token. It is splitting document into tokens based on space (besides other things that I cannot conclude easily). But if there is 2 or more spaces, it will take space as token as well. So my doubt is clear now. Everything can be token, depending how you define it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw = \"Hard to judge whether these sides were good. We were grossed \" \\\n",
    "      \"out by the melted styrofoam and didn't want to eat it for fear of getting sick.\"\n",
    "\n",
    "nlp = English()\n",
    "doc = nlp(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+Hard+=======+.+\n"
     ]
    }
   ],
   "source": [
    "last_word = doc[-1]\n",
    "first_word = doc[0]\n",
    "print(\"+\" + first_word.text + \"+\" + \"=======\" + \"+\" + last_word.text + \"+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Type\n",
    "type(last_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Properties\n",
    "first_word.is_bracket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every token in our document has some characteristics that are know in spaCy as **lexical attributes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(first_word.is_digit)\n",
    "print(last_word.is_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents and spans\n",
    "\n",
    "Two tokens or a sequence of them can be referred to as a $\\textbf{span}$. In some NLP tasks, spans are very relevant. For instance, in Question Answering (QA), obtaining the correct span that answers a query is crucial for the task itself. With spaCy, we can also define spans and use their `lexical attributes` in the same way we can do it for a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "span = doc[4:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "these sides were good.\n"
     ]
    }
   ],
   "source": [
    "print(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'these sides were good.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "whether"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a part-of-speech tag: \n"
     ]
    }
   ],
   "source": [
    "# This cell is reserved for you to explore more about lexical attributes on the previous text. \n",
    "# Check this link: https://spacy.io/api/token for more attributes.\n",
    "third_word = doc[3]\n",
    "print(\"Here is a part-of-speech tag:\", third_word.pos_) # Why is it empty? => Because that word is not part of dataset or similar (check this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get a bit deeper into attributes\n",
    "\n",
    "Working with language requires most of the time math to solve problems. As an example, we can decide if a the word _tweet_ refers to a noun or to a verb by counting.\n",
    "\n",
    "Knowing the context of a word and counting how often our desired word appears after a verb or after a noun would give us the probability that we are searching for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can we include statistics in spaCy?\n",
    "\n",
    "The good news is that spaCy provides pre-trained models that we can use depending on our necessities. There is an offer of small, medium and large models for different languages. Having such a model, we can use attributes in context. \n",
    "\n",
    "But what exactly is contained in a pre-trained model? \n",
    "\n",
    "It contains a vocabulary of the words used to train our model, their weights and meta-information useful for spaCy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download and use a small model for English.\n",
    "\n",
    "Please run the following line in a separate cell\n",
    "\n",
    "`!python -m spacy download en_core_web_sm`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we load a spaCy model?\n",
    "\n",
    "Loading the model is as simple as telling spaCy the name of the model to load."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we already know what to do..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting sick\n"
     ]
    }
   ],
   "source": [
    "# It's your turn to create a new document of our English text \n",
    "# and define a span for its last two words excluding the dot.\n",
    "\n",
    "raw = \"Hard to judge whether these sides were good. We were grossed \" \\\n",
    "      \"out by the melted styrofoam and didn't want to eat it for fear of getting sick.\"\n",
    "\n",
    "# new_doc =\n",
    "# last_span = \n",
    "\n",
    "new_doc = nlp(raw)\n",
    "\n",
    "word_two = new_doc[1]\n",
    "last_span = new_doc[-3:-1]\n",
    "print(last_span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.span.Span'>\n",
      "<class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "print(type(new_doc))\n",
    "print(type(last_span))\n",
    "print(type(word_two))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a part-of-speech tag: SCONJ\n"
     ]
    }
   ],
   "source": [
    "third_word = new_doc[3]\n",
    "print(\"Here is a part-of-speech tag:\", third_word.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'subordinating conjunction'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('SCONJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advcl\n"
     ]
    }
   ],
   "source": [
    "word_judge = new_doc[2]\n",
    "print(word_judge.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting VERB pcomp get\n",
      "sick ADJ acomp sick\n"
     ]
    }
   ],
   "source": [
    "# Now display part-of-speech tags, dependencies and lemma for them.\n",
    "# Token has lexical attributes\n",
    "for token in last_span:\n",
    "    print(token.text, token.pos_, token.dep_, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adjectival complement'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('acomp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure inside spaCy\n",
    "\n",
    "We have seen how to pass raw text to spaCy and process it into lexical features until this point. However, keeping every token for every occurrence in a text is memory expensive. Therefore, spaCy manages everything in a sort of `internal structure`. \n",
    "\n",
    "This structure has three levels or components, the document (doc), a vocabulary called **vocab**, and a look-up table called in spaCy the **string store**. The vocab contains token ids stored as **hashes**. From now on, we will call every entry in vocab a **lexeme**. A look-up table indicates which **token** (from **doc**) corresponds to which lexeme (from **vocab**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does it look like in terms of code?\n",
    "\n",
    "- A document contains tokens with their lexical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting VERB pcomp get\n",
      "<class 'spacy.tokens.token.Token'>\n",
      "--------------------\n",
      "sick ADJ acomp sick\n",
      "<class 'spacy.tokens.token.Token'>\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "for token in last_span:\n",
    "    print(token.text, token.pos_, token.dep_, token.lemma_)\n",
    "    print(type(token))\n",
    "    print('--------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each object in our vocab is a lexeme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.vocab.Vocab'>\n",
      "<class 'spacy.lexeme.Lexeme'>\n",
      "sick 14841597609857081305\n",
      "<class 'spacy.tokens.token.Token'>\n",
      "sick 14841597609857081305\n"
     ]
    }
   ],
   "source": [
    "lexeme = nlp.vocab[last_span[1].text]\n",
    "print(type(nlp.vocab))\n",
    "print(type(lexeme))\n",
    "print(lexeme.text, lexeme.orth)\n",
    "print(type(last_span[1]))\n",
    "print(last_span[1].text, last_span[1].orth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each string representation of a hash id can be search in the string store and viceversa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is my desired string: sick\n",
      "This is my desired hash: 14841597609857081305\n"
     ]
    }
   ],
   "source": [
    "searched_string = nlp.vocab.strings[lexeme.orth]\n",
    "searched_hash = nlp.vocab.strings[lexeme.text]\n",
    "\n",
    "print(\"This is my desired string:\", searched_string)\n",
    "print(\"This is my desired hash:\", searched_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.token.Token'>\n",
      "sick\n",
      "14841597609857081305\n"
     ]
    }
   ],
   "source": [
    "token = new_doc[-2]\n",
    "print(type(new_doc))\n",
    "print(type(token))\n",
    "print(token.text)\n",
    "print(token.orth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.vocab.Vocab'>\n",
      "<class 'spacy.lexeme.Lexeme'>\n",
      "sick\n",
      "14841597609857081305\n"
     ]
    }
   ],
   "source": [
    "lexeme = nlp.vocab[token.text]\n",
    "print(type(nlp.vocab))\n",
    "print(type(lexeme))\n",
    "print(lexeme.text)\n",
    "print(lexeme.orth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is my desired string: sick\n",
      "This is my desired hash: 14841597609857081305\n"
     ]
    }
   ],
   "source": [
    "# Search for token given lexeme in lookup table\n",
    "searched_string = nlp.vocab.strings[lexeme.orth]\n",
    "searched_hash = nlp.vocab.strings[lexeme.text]\n",
    "\n",
    "print(\"This is my desired string:\", searched_string)\n",
    "print(\"This is my desired hash:\", searched_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is my desired string: sick\n",
      "This is my desired hash: 14841597609857081305\n"
     ]
    }
   ],
   "source": [
    "# Search for lexeme given token in lookup table\n",
    "searched_string = nlp.vocab.strings[token.orth]\n",
    "searched_hash = nlp.vocab.strings[token.text]\n",
    "\n",
    "print(\"This is my desired string:\", searched_string)\n",
    "print(\"This is my desired hash:\", searched_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displacy\n",
    "\n",
    "You can also look take a look of visualizations, for instance the graphs presented in our slides for today were created with spaCy\n",
    "\n",
    "Let's take a look of an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_string = \"You can also look take a look of visualizations.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_doc = nlp(raw_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"62b94524254a42d9af970f364ffebe92-0\" class=\"displacy\" width=\"1625\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">You</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">can</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">also</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">look</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">take</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">look</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">of</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">visualizations.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-62b94524254a42d9af970f364ffebe92-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,2.0 575.0,2.0 575.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-62b94524254a42d9af970f364ffebe92-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-62b94524254a42d9af970f364ffebe92-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,89.5 570.0,89.5 570.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-62b94524254a42d9af970f364ffebe92-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-62b94524254a42d9af970f364ffebe92-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-62b94524254a42d9af970f364ffebe92-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-62b94524254a42d9af970f364ffebe92-0-3\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-62b94524254a42d9af970f364ffebe92-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M740.0,266.5 L748.0,254.5 732.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-62b94524254a42d9af970f364ffebe92-0-4\" stroke-width=\"2px\" d=\"M945,264.5 C945,177.0 1090.0,177.0 1090.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-62b94524254a42d9af970f364ffebe92-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,266.5 L937,254.5 953,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-62b94524254a42d9af970f364ffebe92-0-5\" stroke-width=\"2px\" d=\"M770,264.5 C770,89.5 1095.0,89.5 1095.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-62b94524254a42d9af970f364ffebe92-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1095.0,266.5 L1103.0,254.5 1087.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-62b94524254a42d9af970f364ffebe92-0-6\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,177.0 1265.0,177.0 1265.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-62b94524254a42d9af970f364ffebe92-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1265.0,266.5 L1273.0,254.5 1257.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-62b94524254a42d9af970f364ffebe92-0-7\" stroke-width=\"2px\" d=\"M1295,264.5 C1295,177.0 1440.0,177.0 1440.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-62b94524254a42d9af970f364ffebe92-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1440.0,266.5 L1448.0,254.5 1432.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(this_doc, jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching for specific patterns with Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy provides a `Matcher`, which works similar to regular expressions in Python. The difference is that you can search not only the text, but also other token attributes. In this way we could for example differentiate between _break_ being a verb or a noun and search only for noun appearances.\n",
    "\n",
    "Here, we have examples of searching text, lexical attributes for a specific token and lexical attributes in a more general search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Google Inc. is a company that has a big development in NLP. \"\\\n",
    "       \"When users google for a word or any query, their system internally \" \\\n",
    "       \"runs a pipeline in order to process what the person is querying.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "# Those keys in dictionaries (TEXT, LOWER, LEMMA, IS_PUNCT) are lexical attributes of the spaCy Token\n",
    "# Returns span of tokens (if span lenght is 1 then that is individual token)\n",
    "patterns = [\n",
    "    [{'TEXT': 'Google'}, {'TEXT': 'Inc.'}], # .text = (Google AND Inc.)\n",
    "    [{'TEXT': 'Google'}], # Google\n",
    "    [{'TEXT': 'Inc.'}],\n",
    "    [{'LOWER': 'google'}],\n",
    "    [{'LEMMA': 'query'}, {'IS_PUNCT': True}]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add(\"TEST_PATTERNS\", patterns)\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3004906285683798724, 0, 1), (3004906285683798724, 0, 2), (3004906285683798724, 1, 2), (3004906285683798724, 15, 16), (3004906285683798724, 21, 23), (3004906285683798724, 37, 39)]\n",
      "Total of matches found: 6\n"
     ]
    }
   ],
   "source": [
    "print(matches)\n",
    "print(\"Total of matches found:\", len(matches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, what can we do with this output? What does it mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Matcher` returns a list of tuples indicating start and end of each found matched span. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: ['Google', 'Google Inc.', 'Inc.', 'google', 'query,', 'querying.']\n"
     ]
    }
   ],
   "source": [
    "# Display a list of found matches\n",
    "print(\"Matches:\", [doc[start:end].text for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following what we have seen until now, download a German model and create patterns to find several tokens with more than one ocurrence in the text given in following cells. \n",
    "\n",
    "***Hint:*** Notice that models for other languages were trained on news data instead of web data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_nlp = spacy.load('de_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.de.examples import sentences\n",
    "raw_german = sentences[0:5]\n",
    "print(raw_german)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_eau2HNkfO49"
   },
   "source": [
    "# Word embedding using spaCy\n",
    "In this notebook you will find out how to use spaCy to get word vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_i1pPQVLZJY"
   },
   "source": [
    "Loading pre-trained embeddings with spaCy."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "HSQRYygqg2tp"
   },
   "outputs": [],
   "source": [
    "nlp_lg = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.lang.en.English'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "The quick brown fox jumps over the lazy dog\n"
     ]
    }
   ],
   "source": [
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "doc = nlp_lg(text)\n",
    "print(type(nlp_lg))\n",
    "print(type(doc))\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Retrieve the second Token in the Doc object at index 1, and  the first 30 dimensions of its vector representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.token.Token'>\n",
      "quick\n",
      "(96,)\n",
      "<class 'numpy.ndarray'>\n",
      "[ 0.8233709  -1.1887459   0.9393252   1.1702604   0.26628137 -0.00492318\n",
      " -0.36061257 -0.85801125 -0.33466592 -0.6535297   0.18827835  0.17926934\n",
      " -1.456599   -0.3641851  -0.45112038  0.3773813   0.6100651  -0.07231146\n",
      " -1.2368598  -0.6287472  -0.31162527  0.9800494   0.25633457 -0.04945124\n",
      " -0.4890042   0.2708063   0.15849347  0.43778464  0.7010016   0.6340733 ]\n"
     ]
    }
   ],
   "source": [
    "second_token = doc[1]\n",
    "print(type(second_token))\n",
    "print(second_token.text)\n",
    "print(second_token.vector.shape)\n",
    "print(type(second_token.vector))\n",
    "print(second_token.vector[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving word vectors for \"dog\", \"fox\" and \"sun\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.6806675 , -1.2663747 , -0.71255565,  0.22143888,  0.28581634,\n",
       "        0.23924345,  1.2992647 ,  1.0683641 , -0.1666507 , -0.593021  ,\n",
       "        0.20207635, -0.71124184, -0.5710875 , -0.2685267 , -0.5052826 ,\n",
       "        0.60505986, -1.5851773 , -1.6874862 ,  0.7026561 ,  0.60366225,\n",
       "        0.3043416 ,  1.3963956 , -0.056483  , -0.6299367 ,  0.09717859,\n",
       "        0.5463655 ,  0.36506647,  0.73901325, -0.16906115,  0.35410628,\n",
       "        0.33770823, -0.7352046 ,  1.6755302 ,  0.48371333,  0.0184648 ,\n",
       "       -0.92315257,  0.6245377 ,  0.11393103,  0.8193037 , -0.01115507,\n",
       "       -0.49064368, -0.30198318,  0.43095675, -0.05127436, -0.11000359,\n",
       "       -0.64060974, -1.4619632 ,  0.85834503, -0.4855454 ,  0.01614086,\n",
       "       -0.10124743,  1.2471893 ,  0.7936216 , -0.49573362,  1.0994403 ,\n",
       "       -0.22723481,  1.289906  , -0.8966851 , -0.07580797, -0.6877714 ,\n",
       "       -0.9954758 , -0.70957065, -0.3484952 , -0.35015944,  0.6045856 ,\n",
       "        0.21346438, -0.22741812,  0.12088367,  0.8521288 ,  0.11694434,\n",
       "        0.42555034,  0.8777968 ,  1.9081106 , -0.62818205,  0.2991566 ,\n",
       "       -0.26263964, -0.4627701 , -0.11653326,  0.6030469 , -1.1594303 ,\n",
       "       -0.3226232 , -0.1124312 , -1.5564214 , -0.44001883,  0.8246197 ,\n",
       "        1.0218511 ,  0.31111258, -0.46998724, -2.129951  ,  1.1728595 ,\n",
       "       -1.2842656 , -1.1179041 ,  1.596463  ,  0.51193094,  0.22032768,\n",
       "        1.6200272 ], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_lg('dog').vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96,)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_lg('dog').vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In spaCy, the vector representation for the entire Doc is calculated by averaging the vectors for each Token in the Doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96,)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.88126457, -0.6202806 , -1.5005524 , -0.40187687,  0.36590713,\n",
       "       -0.30607873, -0.21017855,  1.8757141 , -0.17634499, -0.03765219,\n",
       "        0.82590777, -0.08941758, -0.99377054, -0.40427023, -0.37538618,\n",
       "        0.912148  , -1.1020703 ,  0.1130835 ,  0.85248625,  0.6235067 ,\n",
       "       -2.0766525 ,  0.06002179, -0.45891204, -0.04191741, -0.09031391,\n",
       "        0.94857234, -0.17983632,  1.2523849 , -0.8432184 ,  0.16701965,\n",
       "       -0.34979108,  0.59383965,  0.7371594 ,  0.9947243 , -0.339404  ,\n",
       "       -0.8623344 , -0.2672234 ,  1.7776172 ,  1.0509973 , -0.6767344 ,\n",
       "        0.02610242, -0.8435878 ,  0.34696946, -0.8810245 ,  0.06217456,\n",
       "       -0.63664925, -0.12952982, -0.15066275, -0.25706494,  0.6818833 ,\n",
       "        0.6012949 ,  0.22682005,  0.04043951,  0.71112496,  0.4236604 ,\n",
       "       -0.9417363 ,  2.9450629 , -0.8021494 ,  0.02782455,  1.0667045 ,\n",
       "       -0.32455254, -1.1640505 , -0.05386074, -0.91956043,  0.9660482 ,\n",
       "        0.32777134, -0.54104346,  0.40791127, -0.92241853,  1.6783303 ,\n",
       "        0.75316286,  1.5443585 , -0.7938992 ,  0.06903949,  0.09393178,\n",
       "        0.26469985,  0.0123996 , -0.81592757, -1.2138216 , -0.7849026 ,\n",
       "       -0.7067964 ,  0.255246  , -0.3931991 ,  0.7764721 , -0.7483736 ,\n",
       "        0.22444023, -0.3971338 , -0.21467687, -1.3432385 ,  0.28536534,\n",
       "       -0.09133293,  0.1071112 ,  1.4145058 ,  1.0265377 ,  0.828593  ,\n",
       "        0.5461384 ], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heeiiyy = nlp_lg(\"heeiiyy\")\n",
    "heeiiyy.vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1DIfQu3Lix4"
   },
   "source": [
    "Task 2: Compare the similarity of words \"dog\" and \"fox\" & \"dog\" and \"sun\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "fD_Pi9gZ88O0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity of dog and fox: 0.7081974719792407\n",
      "Similarity of dog and sun: 0.7552881594262363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-55-b8c61bd210f6>:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print(\"Similarity of dog and fox: \" + str(nlp_lg(\"dog\").similarity(nlp_lg(\"fox\"))))\n",
      "<ipython-input-55-b8c61bd210f6>:2: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print(\"Similarity of dog and sun: \" + str(nlp_lg(\"dog\").similarity(nlp_lg(\"sun\"))))\n"
     ]
    }
   ],
   "source": [
    "print(\"Similarity of dog and fox: \" + str(nlp_lg(\"dog\").similarity(nlp_lg(\"fox\"))))\n",
    "print(\"Similarity of dog and sun: \" + str(nlp_lg(\"dog\").similarity(nlp_lg(\"sun\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'whatlies'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwhatlies\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SpacyLanguage\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'whatlies'"
     ]
    }
   ],
   "source": [
    "from whatlies.language import SpacyLanguage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the spaCy language model under 'nlp_lg' into the\n",
    "# whatlies SpacyLanguage class and assign the result \n",
    "# under the variable 'language_model'\n",
    "language_model = SpacyLanguage(nlp_lg)\n",
    "\n",
    "# Call the variable to examine the output\n",
    "language_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a SpacyLanguage object that wraps a spaCy Language object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = language_model[['fox', 'dog', 'sun']]\n",
    "\n",
    "# Plot the EmbSet\n",
    "embeddings.plot(kind='arrow', color='red', x_axis=0, y_axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_king = wv_model['king']\n",
    "len(vec_king)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\n"
     ]
    }
   ],
   "source": [
    "print(wv_model.doesnt_match(['fire', 'water', 'land', 'sea', 'air', 'car']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vehicle', 0.7821096181869507),\n",
       " ('cars', 0.7423831224441528),\n",
       " ('SUV', 0.7160962820053101),\n",
       " ('minivan', 0.6907036900520325),\n",
       " ('truck', 0.6735789775848389),\n",
       " ('Car', 0.6677608489990234),\n",
       " ('Ford_Focus', 0.6673202514648438),\n",
       " ('Honda_Civic', 0.6626849174499512),\n",
       " ('Jeep', 0.651133120059967),\n",
       " ('pickup_truck', 0.6441438794136047)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims = wv_model.most_similar('car', topn=10)  # get similar words\n",
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16958451"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_model.similarity(w1='car', w2='ship')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4243558"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_model.similarity(w1='car', w2='airplane')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.325606"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_model.similarity(w1='ship', w2='airplane')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67357904"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_model.similarity(w1='car', w2='truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
