{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92073528",
   "metadata": {
    "id": "92073528"
   },
   "source": [
    "# Introduction to Natural Language Processing: Assignment 2\n",
    "\n",
    "In this exercise we'll practice training and testing classifiers.\n",
    "\n",
    "- You can use any built-in Python packages, scikit-learn and Pandas.\n",
    "- Please comment your code\n",
    "- Submissions are due Thursday at 23:59 and should be submitted **ONLY** on eCampus: **Assignmnets >> Student Submissions >> Assignment 2 (Deadline: 21.11.2023, at 23:59)**\n",
    "- Name the file aproppriately \"Assignment_2_\\<Your_Name\\>.ipynb\".\n",
    "- Please use relative paths, your code should run on my computer if the notebook and the file are both in the same directory.\n",
    "\n",
    "Example: file_name = polarity.txt >> **DON'T use:** /Users/ComputerName/Username/Documents/.../polarity.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85f6d30e-7eb2-4244-b898-5090daa9f586",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/faris/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import naive_bayes\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef5be54",
   "metadata": {
    "id": "7ef5be54"
   },
   "source": [
    "### Task 1.1 (2 point)\n",
    "\n",
    "Create a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60c659fa-ec58-471a-affd-66f32a462e2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = \"./polarity.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a259fbec",
   "metadata": {
    "id": "a259fbec",
    "tags": []
   },
   "outputs": [],
   "source": [
    "polarity_df = pd.read_csv(file_path, sep='\\t', names=['Text', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76b804f3-3dde-42f7-a8fa-bc95a8379068",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>every now and then a movie comes along from a ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mtv films' _election , a high school comedy st...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>did anybody know this film existed a week befo...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the plot is deceptively simple .</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Label\n",
       "0  every now and then a movie comes along from a ...   pos\n",
       "1  mtv films' _election , a high school comedy st...   pos\n",
       "2  did anybody know this film existed a week befo...   pos\n",
       "3                  the plot is deceptively simple .    pos"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarity_df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b505bc8",
   "metadata": {
    "id": "7b505bc8"
   },
   "source": [
    "### Task 1.2 (2 point)\n",
    "\n",
    "Create a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\n",
    "\n",
    "Hint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b671658",
   "metadata": {
    "id": "0b671658",
    "tags": []
   },
   "outputs": [],
   "source": [
    "polarity_df['numLabel'] = polarity_df['Label'].apply(lambda x: 1 if x == 'pos' else 0)\n",
    "polarity_df.drop('Label', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "041655c3-ad1c-4a8a-9e2b-ed6c39bdb7f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>numLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>every now and then a movie comes along from a ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mtv films' _election , a high school comedy st...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>did anybody know this film existed a week befo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the plot is deceptively simple .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  numLabel\n",
       "0  every now and then a movie comes along from a ...         1\n",
       "1  mtv films' _election , a high school comedy st...         1\n",
       "2  did anybody know this film existed a week befo...         1\n",
       "3                  the plot is deceptively simple .          1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarity_df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dcdd4c",
   "metadata": {
    "id": "53dcdd4c"
   },
   "source": [
    "### Task 2 (8 points)\n",
    "\n",
    "Write a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n",
    "1. Text\n",
    "2. Count_Vector\n",
    "3. Probability\n",
    "\n",
    "Example:\n",
    "\n",
    "For the line: `This document is the second document.`\n",
    "\n",
    "The row in the csv file should contain:\n",
    "`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n",
    "\n",
    "**Note**:\n",
    "\n",
    "1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n",
    "\n",
    "2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n",
    "\n",
    "```\n",
    "import re\n",
    "TEXT = \"Hey, - How are you doing today!?\"\n",
    "words_list = re.findall(r\"[\\w']+\", TEXT)\n",
    "print(words_list)\n",
    "```\n",
    "\n",
    "3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n",
    "\n",
    "4. Please don't upload the output file. Your function should generate the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc49a6e0-a371-479a-98fa-b99d3ad3942d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e943a00-e3ff-40f3-9919-a03e6299a6ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6062c95a-3bc4-4251-b9ac-c74b0f7cbe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate bag-of-words i.e. features (same as \"count_vectorizer.vocabulary_.keys()\" from tutorial)\n",
    "# From corpus => list of documents/sentences\n",
    "def unique_words(corpus):\n",
    "    all_words = [re.findall(r\"[\\w']+\", sentence) for sentence in corpus]\n",
    "    unique_words_set = set(word.lower() for words in all_words for word in words)\n",
    "    unique_words_list = list(unique_words_set)\n",
    "    return unique_words_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92eac6d7-30cb-4d6f-be78-88a79b356c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate bag-of-words i.e. features (same as \"count_vectorizer.vocabulary_.keys()\" from tutorial)\n",
    "# From plain text\n",
    "def unique_words2(text):\n",
    "    words_list = re.findall(r\"[\\w']+\", text)\n",
    "    unique_words_set = set(word.lower() for word in words_list)\n",
    "    unique_words_list = list(unique_words_set)\n",
    "    return unique_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0c2ce8e-cd00-4efb-9c1d-3ded30b247a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_probability(sentence):\n",
    "    word_list = re.findall(r\"[\\w']+\", sentence)\n",
    "    word_list = list(word.lower() for word in word_list)\n",
    "    \n",
    "    # Calculate word frequencies (returns dictionary where key is word, value is frequency)\n",
    "    word_counts = Counter(word_list)\n",
    "    \n",
    "    total_words = len(word_list)\n",
    "    \n",
    "    # Calculate probabilities for each word\n",
    "    word_probabilities = [word_counts[word] / total_words for word in word_list]\n",
    "    \n",
    "    return word_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d11b7e3-7407-47f2-bd28-81684d78e875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vectorizer(sentence, unique_words):\n",
    "    word_list = re.findall(r\"[\\w']+\", sentence)\n",
    "    word_list = list(word.lower() for word in word_list)\n",
    "    \n",
    "    # Count occurrences of UNIQUE words (i.e of all vocabulary words / features / bag-of-words)\n",
    "    feature_vector = [word_list.count(word.lower()) for word in unique_words]\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064a9d6d-6804-48f3-b727-674cb10d7641",
   "metadata": {},
   "source": [
    "#### Note: DataFrame is returned as the output from the function, and .csv file is generated during execution of this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1a09321",
   "metadata": {
    "id": "a1a09321"
   },
   "outputs": [],
   "source": [
    "def create_count_and_probability(file_name):\n",
    "    # Read file into variable\n",
    "    with open(file_name, 'r') as file:\n",
    "        text = file.read()\n",
    "        \n",
    "    # Use sentence tokenizer from nltk to get sentences from string text. Output is list of sentences (i.e. corpus)\n",
    "    corpus = sent_tokenize(text)\n",
    "    \n",
    "    \n",
    "    # Get list of unique words from whole text => This words represent features (Bag of Words)\n",
    "    # I will assume that capitalized and non-capitalized words are same (so I will lower all words)\n",
    "    bag_of_words = unique_words(corpus)\n",
    "    \n",
    "    # Create an empty DataFrame\n",
    "    df = pd.DataFrame(columns=['Sentence', 'Count_Vector', 'Probability'])\n",
    "    \n",
    "    \n",
    "    for sentence in corpus:\n",
    "        # For each sentece (i.e. sample) calculate how many times unique words appears in that sentence (i.e. calculate values for all features)\n",
    "        feature_vector = count_vectorizer(sentence, bag_of_words)\n",
    "        word_counts = words_probability(sentence)\n",
    "        df = df.append({'Sentence': sentence, 'Count_Vector': feature_vector, 'Probability': word_counts}, ignore_index=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    df.to_csv(\"corpus_parsed_csv.csv\", index=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40f1acab-97f4-4abc-9367-96fb8c0ff686",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = create_count_and_probability(\"corpus.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc48d8d-9511-483f-a0ad-51c37af1bcf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f7b01d-d0e3-4430-9916-44926bc2beda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bba52d-09b3-415a-a50c-a5316dba82f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41c56eef-5e75-41b5-a05c-f68d0b7d98fd",
   "metadata": {
    "id": "41c56eef-5e75-41b5-a05c-f68d0b7d98fd"
   },
   "source": [
    "### Task 3 (8 points)\n",
    "\n",
    "The goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\n",
    "\n",
    "a) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n",
    "\n",
    "|Document                             |Class |\n",
    "| ------------------------------------|----- |\n",
    "|PM denies knowledge of AWB kickbacks | rural |\n",
    "|The crocodile ancestor fossil, found...| science |\n",
    "\n",
    "\n",
    "b) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n",
    "\n",
    "- naive_bayes.GaussianNB()\n",
    "- svm.LinearSVC().\n",
    "\n",
    "c) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n",
    "\n",
    "**Hints:**\n",
    "1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n",
    "2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc445f1-d1ba-423d-98cc-c26c5b3b5725",
   "metadata": {},
   "source": [
    "## Load .txt files into the DataFrame (create dataset)\n",
    "\n",
    "Each sentence is splitted by new line, and I am going to load it like that. After that for those two lists, I will append each element to the empty DataFrame using $\\textbf{.loc}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad8f2428-6497-4378-9086-c2cc2ac155dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "science_path = \"./science.txt\"\n",
    "rural_path = \"./rural.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f4b527a-0d04-4a0e-9281-fec9e0cd0ec0",
   "metadata": {
    "id": "4f4b527a-0d04-4a0e-9281-fec9e0cd0ec0"
   },
   "outputs": [],
   "source": [
    "with open(science_path, 'r') as file:\n",
    "        science = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "815a8070-f2b3-48c6-9280-e2ce39373263",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(rural_path, 'r') as file:\n",
    "        rural = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "845b7329-1ff8-4f2a-b82f-db1ad8af5bdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#science_corpus = re.split(r'[\\n]', science)\n",
    "##rural_corpus = re.split(r'[\\n]', rural)\n",
    "science_corpus = science.split('\\n')[:-1]\n",
    "rural_corpus = rural.split('\\n')[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9ebc224-201a-4d8a-aaa5-268949dce575",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1122"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(science_corpus) + len(rural_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04f14537-f120-4723-bb61-664e201375ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_dataset = pd.DataFrame(columns=['Document', 'Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f29d60fc-3337-461b-8806-4f608565b907",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(science_corpus)):\n",
    "    df_dataset.loc[i,'Document'] = science_corpus[i]\n",
    "    df_dataset.loc[i,'Class'] = \"science\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e27e2aaf-5bfe-4a7c-9553-1c2faf31c0f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prev_length = len(science_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c317ee8-8bbd-4ad4-acfc-839a8e2226e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for j in range(len(rural_corpus)):\n",
    "    df_dataset.loc[prev_length + j,'Document'] = rural_corpus[j]\n",
    "    df_dataset.loc[prev_length + j,'Class'] = \"rural\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39ce7891-130d-4057-9429-36b24ff9cbf3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cystic fibrosis affects 30,000 children and yo...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Inhaling the mists of salt water can reduce th...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>That's the conclusion of two studies published...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>They found that inhaling a mist with a salt co...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cystic fibrosis, a progressive and frequently ...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Document    Class\n",
       "0  Cystic fibrosis affects 30,000 children and yo...  science\n",
       "1  Inhaling the mists of salt water can reduce th...  science\n",
       "2  That's the conclusion of two studies published...  science\n",
       "3  They found that inhaling a mist with a salt co...  science\n",
       "4  Cystic fibrosis, a progressive and frequently ...  science"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b5395c-9dd9-4d9c-b0cc-31edf2650d3c",
   "metadata": {},
   "source": [
    "## Split dataset into train and test using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22faf417-c540-42dc-be91-a8899ce596e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_train, text_test, label_train, label_test = train_test_split(df_dataset['Document'], df_dataset['Class'], \n",
    "                                                                  test_size=0.30, \n",
    "                                                                  random_state=1234, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cac9ae3-1308-41fc-8a4b-f2e0b2e56f46",
   "metadata": {},
   "source": [
    "## Creating Features (TF-IDF-Vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b8274fe-578f-4124-946f-e873d8f3bbcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Instance of TfidVectorizer\n",
    "tf_idf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit that instance to train data\n",
    "tf_idf_vectorizer.fit(text_train)\n",
    "\n",
    "# Transform texual data to numeric data (BoW Features)\n",
    "train_bow_features = tf_idf_vectorizer.transform(text_train)\n",
    "test_bow_features = tf_idf_vectorizer.transform(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd6654d9-6da7-4f79-92d8-ae705d2c064c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert from sparse matrix to dense numpy matrix\n",
    "train_bow_features = train_bow_features.toarray()\n",
    "test_bow_features = test_bow_features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae1586e-31fc-4b8a-88db-b1c4665de3a2",
   "metadata": {},
   "source": [
    "## Accuracy, Precision, Recall, F1 Score and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6039016e-c756-448d-b473-c99b42c71cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_to_numerical(categorical):\n",
    "    numerical = [1 if label == 'science' else 0 for label in categorical]\n",
    "    return np.array(numerical)\n",
    "\n",
    "def calculate_accuracy(predicted_labels_categorical, gt_labels_categorical):\n",
    "    predicted_labels = categorical_to_numerical(predicted_labels_categorical)\n",
    "    gt_labels = categorical_to_numerical(gt_labels_categorical)\n",
    "    \n",
    "    correct_predictions = np.sum(predicted_labels == gt_labels)\n",
    "    total_samples = len(gt_labels)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return accuracy\n",
    "\n",
    "def calculate_precision(predicted_labels_categorical, gt_labels_categorical):\n",
    "    predicted_labels = categorical_to_numerical(predicted_labels_categorical)\n",
    "    gt_labels = categorical_to_numerical(gt_labels_categorical)\n",
    "    \n",
    "    true_positives = np.sum((predicted_labels == 1) & (gt_labels == 1))\n",
    "    false_positives = np.sum((predicted_labels == 1) & (gt_labels == 0))\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    return precision\n",
    "\n",
    "def calculate_recall(predicted_labels_categorical, gt_labels_categorical):\n",
    "    predicted_labels = categorical_to_numerical(predicted_labels_categorical)\n",
    "    gt_labels = categorical_to_numerical(gt_labels_categorical)\n",
    "    \n",
    "    true_positives = np.sum((predicted_labels == 1) & (gt_labels == 1))\n",
    "    false_negatives = np.sum((predicted_labels == 0) & (gt_labels == 1))\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    return recall\n",
    "\n",
    "def calculate_f1(predicted_labels_categorical, gt_labels_categorical):    \n",
    "    precision = calculate_precision(predicted_labels_categorical, gt_labels_categorical)\n",
    "    recall = calculate_recall(predicted_labels_categorical, gt_labels_categorical)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "def calculate_confusion_matrix(predicted_labels_categorical, gt_labels_categorical):\n",
    "    predicted_labels = categorical_to_numerical(predicted_labels_categorical)\n",
    "    gt_labels = categorical_to_numerical(gt_labels_categorical)\n",
    "    \n",
    "    true_positives = np.sum((predicted_labels == 1) & (gt_labels == 1))\n",
    "    false_positives = np.sum((predicted_labels == 1) & (gt_labels == 0))\n",
    "    true_negatives = np.sum((predicted_labels == 0) & (gt_labels == 0))\n",
    "    false_negatives = np.sum((predicted_labels == 0) & (gt_labels == 1))\n",
    "\n",
    "    confusion_matrix = np.array([[true_negatives, false_positives], [false_negatives, true_positives]])\n",
    "\n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0c0e6e-ca72-4197-8baa-0a0c8910eb46",
   "metadata": {},
   "source": [
    "## ML Model - LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05830063-48ee-404a-b8cd-d0a24ac37102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/faris/miniconda3/envs/nlp/lib/python3.8/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "svm_classifier = svm.LinearSVC()\n",
    "svm_classifier.fit(train_bow_features, label_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f96ed7a9-2c45-4843-97ae-f61e97d8dea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label_test = svm_classifier.predict(test_bow_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "69347e28-4a44-4ea9-9196-08f11189dd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_svc = calculate_accuracy(predicted_label_test, label_test)\n",
    "precision_svc = calculate_precision(predicted_label_test, label_test)\n",
    "recall_svc = calculate_recall(predicted_label_test, label_test)\n",
    "f1_svc = calculate_f1(predicted_label_test, label_test)\n",
    "confusion_matrix_svc = calculate_confusion_matrix(predicted_label_test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a633a749-dbcf-42d3-a92f-9fec1b35e192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LinearSVC:  0.9406528189910979\n",
      "Precision of LinearSVC:  0.9273743016759777\n",
      "Recall of LinearSVC:  0.9595375722543352\n",
      "F1 Score of LinearSVC:  0.9431818181818181\n",
      "Confusion Matrix of LinearSVC:  [[151  13]\n",
      " [  7 166]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of LinearSVC: \", accuracy_svc)\n",
    "print(\"Precision of LinearSVC: \", precision_svc)\n",
    "print(\"Recall of LinearSVC: \", recall_svc)\n",
    "print(\"F1 Score of LinearSVC: \", f1_svc)\n",
    "print(\"Confusion Matrix of LinearSVC: \", confusion_matrix_svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c115e838-d43d-4bc1-8ef8-922bd2cbe17c",
   "metadata": {},
   "source": [
    "## ML Model - Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "389fb2bb-55c9-495e-9161-9bbecdc3a7a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "naive_bayes_classifier = naive_bayes.GaussianNB()\n",
    "naive_bayes_classifier.fit(train_bow_features, label_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "124efcb8-88bd-46be-8b3c-b4632eaa1b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label_test = naive_bayes_classifier.predict(test_bow_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a343ef20-00be-4d2f-b273-1beccbbae2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_gnb = calculate_accuracy(predicted_label_test, label_test)\n",
    "precision_gnb = calculate_precision(predicted_label_test, label_test)\n",
    "recall_gnb = calculate_recall(predicted_label_test, label_test)\n",
    "f1_gnb = calculate_f1(predicted_label_test, label_test)\n",
    "confusion_matrix_gnb = calculate_confusion_matrix(predicted_label_test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf783fcf-a234-404e-97cd-a9946535a363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of GNB:  0.9050445103857567\n",
      "Precision of GNB:  0.9122807017543859\n",
      "Recall of GNB:  0.9017341040462428\n",
      "F1 Score of GNB:  0.9069767441860466\n",
      "Confusion Matrix of GNB:  [[149  15]\n",
      " [ 17 156]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of GNB: \", accuracy_gnb)\n",
    "print(\"Precision of GNB: \", precision_gnb)\n",
    "print(\"Recall of GNB: \", recall_gnb)\n",
    "print(\"F1 Score of GNB: \", f1_gnb)\n",
    "print(\"Confusion Matrix of GNB: \", confusion_matrix_gnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d61740-4b7b-4aa9-a113-99010c8aeb4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdef9bf5-3d09-4cab-88ae-85d3742ec982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65d8108-187a-4bd9-8dfa-c9a3663198ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
